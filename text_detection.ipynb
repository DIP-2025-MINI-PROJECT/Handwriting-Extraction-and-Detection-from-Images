import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.callbacks import ReduceLROnPlateau
import numpy as np
import scipy.io as sio
import os
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, confusion_matrix
from itertools import cycle
print("Loading .mat file from Kaggle dataset...")
mat_file_path = "/kaggle/input/emnist-byclass-mat/emnist-byclass.mat"
if not os.path.exists(mat_file_path):
 print("="*50)
 print("ERROR: Dataset not found.")
 print("Please make sure you have added the 'EMNIST dataset in .mat format' to this notebook.")
 print("="*50)
else:
 mat = sio.loadmat(mat_file_path)
 x_train = mat['dataset']['train'][0,0]['images'][0,0]
 y_train_labels = mat['dataset']['train'][0,0]['labels'][0,0]
 x_test = mat['dataset']['test'][0,0]['images'][0,0]
 y_test_labels = mat['dataset']['test'][0,0]['labels'][0,0]
 print("Preprocessing data...")
 x_train = x_train.reshape((-1, 28, 28)).transpose((0, 2, 1))
 x_test = x_test.reshape((-1, 28, 28)).transpose((0, 2, 1))
 x_train = np.expand_dims(x_train, axis=-1)
 x_test = np.expand_dims(x_test, axis=-1)
 print("Applying normalization...")
 x_train = x_train.astype('float32') / 255.0
 x_test = x_test.astype('float32') / 255.0
 num_classes = 62
 y_train_one_hot = keras.utils.to_categorical(y_train_labels, num_classes)
 y_test_one_hot = keras.utils.to_categorical(y_test_labels, num_classes)
 print(f"Training data shape: {x_train.shape}")
 print(f"Test data shape: {x_test.shape}")
 print("Using 100% of the data for maximum accuracy.")
 metrics_to_track = [
 'accuracy',
 tf.keras.metrics.AUC(name='auc')
 ]
 print("Building model...")
 input_shape = (28, 28, 1)
 model = keras.Sequential(
 [
 layers.Input(shape=input_shape),
 layers.Conv2D(32, kernel_size=(3, 3), padding='same'),
 layers.BatchNormalization(),
 layers.ReLU(),
 layers.Conv2D(32, kernel_size=(3, 3), padding='same'),
 layers.BatchNormalization(),
 layers.ReLU(),
 layers.MaxPooling2D(pool_size=(2, 2), strides=2),
 layers.Dropout(0.25),
 layers.Conv2D(64, kernel_size=(3, 3), padding='same'),
 layers.BatchNormalization(),
 layers.ReLU(),
 layers.Conv2D(64, kernel_size=(3, 3), padding='same'),
 layers.BatchNormalization(),
 layers.ReLU(),
 layers.MaxPooling2D(pool_size=(2, 2), strides=2),
 layers.Dropout(0.25),
 layers.Flatten(),
 layers.Dense(512),
 layers.BatchNormalization(),
 layers.ReLU(),
 layers.Dropout(0.5),
 layers.Dense(num_classes, activation="softmax"),
 ]
 )
 data_augmentation = keras.Sequential([
 layers.RandomRotation(factor=(-0.02, 0.02)),
 layers.RandomTranslation(height_factor=0.07, width_factor=0.07),
 layers.RandomZoom(height_factor=(-0.1, 0.1))
 ])
 train_model = keras.Sequential([
 layers.Input(shape=input_shape),
 data_augmentation,
 model
 ])
 model.compile(
 loss="categorical_crossentropy",
 optimizer="adam",
 metrics=metrics_to_track
 )
 train_model.compile(
 loss="categorical_crossentropy",
 optimizer="adam",
 metrics=metrics_to_track
 )
 model.summary()
 early_stopping = keras.callbacks.EarlyStopping(
 monitor='val_accuracy',
 patience=5,
 restore_best_weights=True
 )
 reduce_lr = ReduceLROnPlateau(
 monitor='val_loss',
 factor=0.2,
 patience=2,
 min_lr=1e-6,
 verbose=1
 )
 batch_size = 128
 epochs = 30
 print(f"Using batch size: {batch_size}")
 history = train_model.fit(
 x_train, y_train_one_hot,
 batch_size=batch_size,
 epochs=epochs,
 validation_data=(x_test, y_test_one_hot),
 callbacks=[early_stopping, reduce_lr]
 )
 print("\n\n--- Training Complete ---")
 print("Plotting Training vs. Validation curves...")
 fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))
 ax1.plot(history.history['accuracy'])
 ax1.plot(history.history['val_accuracy'])
 ax1.set_title('Model Accuracy')
 ax1.set_ylabel('Accuracy')
 ax1.set_xlabel('Epoch')
 ax1.legend(['Train', 'Validation'], loc='upper left')
 ax2.plot(history.history['loss'])
 ax2.plot(history.history['val_loss'])
 ax2.set_title('Model Loss')
 ax2.set_ylabel('Loss')
 ax2.set_xlabel('Epoch')
 ax2.legend(['Train', 'Validation'], loc='upper left')
 plt.tight_layout()
 plt.show()
print("\nEvaluating final model on the Test Set...")
print("Calculating metrics (Accuracy, AUC, Sensitivity, Specificity)...")
y_pred_proba = train_model.predict(x_test, batch_size=batch_size)
y_pred_classes = np.argmax(y_pred_proba, axis=1)
from sklearn.metrics import accuracy_score
manual_accuracy = accuracy_score(y_test_labels, y_pred_classes)
cm = confusion_matrix(y_test_labels, y_pred_classes)
per_class_sensitivity = []
per_class_specificity = []
num_classes = 62
epsilon = 1e-6
for i in range(num_classes):
 tp_i = cm[i, i]
 fn_i = np.sum(cm[i, :]) - tp_i
 fp_i = np.sum(cm[:, i]) - tp_i
 tn_i = np.sum(cm) - (tp_i + fn_i + fp_i)
 per_class_sensitivity.append(tp_i / (tp_i + fn_i + epsilon))
 per_class_specificity.append(tn_i / (tn_i + fp_i + epsilon))
sensitivity = np.mean(per_class_sensitivity)
specificity = np.mean(per_class_specificity)
fpr_micro, tpr_micro, _ = roc_curve(y_test_one_hot.ravel(), y_pred_proba.ravel())
roc_auc_micro = auc(fpr_micro, tpr_micro)
fpr_macro = {}
tpr_macro = {}
for i in range(num_classes):
 fpr_macro[i], tpr_macro[i], _ = roc_curve(y_test_one_hot[:, i], y_pred_proba[:, i])
all_fpr = np.unique(np.concatenate([fpr_macro[i] for i in range(num_classes)]))
mean_tpr = np.zeros_like(all_fpr)
for i in range(num_classes):
 mean_tpr += np.interp(all_fpr, fpr_macro[i], tpr_macro[i])
mean_tpr /= num_classes
roc_auc_macro_avg = auc(all_fpr, mean_tpr)
print("\n--- Final Test Metrics (Manually Calculated) ---")
print(f"Test Accuracy: {manual_accuracy * 100:.2f}%")
print(f"Macro-Avg AUC: {roc_auc_macro_avg:.4f}")
print(f"Micro-Avg AUC: {roc_auc_micro:.4f}")
print(f"Macro-Avg Recall: {sensitivity:.4f}")
print(f"Macro-Avg Spec: {specificity:.4f}")
print("-------------------------------------------------")
print("\nCalculating and plotting Test Set AUC-ROC Curve...")
plt.figure(figsize=(8, 6))
plt.plot(fpr_micro, tpr_micro,
 label=f'Micro-average ROC (area = {roc_auc_micro:0.3f})',
 color='deeppink', linestyle=':', linewidth=4)
plt.plot(all_fpr, mean_tpr,
 label=f'Macro-average ROC (area = {roc_auc_macro_avg:0.3f})',
 color='navy', linestyle=':', linewidth=4)
plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Multi-class Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid()
plt.show()
print("Saving trained model to 'saved_model_directory'...")
train_model.export("saved_model_directory")
print("Model saved successfully.")
!pip install tf2onnx -U
print("Installing converter and converting model from file...")
saved_model_dir = "saved_model_directory"
output_path = "/kaggle/working/emnist_model.onnx"
print(f"Loading and converting model from {saved_model_dir}...")
!python -m tf2onnx.convert --saved-model {saved_model_dir} --output {output_path} --opset 13
print(f"\nModel saved to {output_path}")
print("You can now download this file from the 'Output' section in the right-hand panel.")
